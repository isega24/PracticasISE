\input{preambuloSimple.tex}

\title{	
	\normalfont \normalsize 
	\textsc{{\bf Ingeniería de Servidores (2015-2016)} \\ Grado en Ingeniería Informática y Matemáticas \\ Universidad de Granada} \\ [25pt] % Your university, school and/or department name(s)
	\horrule{0.5pt} \\[0.4cm] % Thin top horizontal rule
	\huge Memoria Práctica 4 \\ % The assignment title
	\horrule{2pt} \\[0.5cm] % Thick bottom horizontal rule
}

\author{Iván Sevillano García} % Nombre y apellidos

\date{\normalsize\today} % Incluye la fecha actual

\begin{document}

\maketitle % Muestra el Título

\newpage %inserta un salto de página

\tableofcontents % para generar el índice de contenidos

\newpage

\section{Benchmarks populares}

\subsection{Phoronis suite}

\begin{itemize}
	\item \textbf{Instale la aplicación. ¿Qué comando permite listar los benchmarks disponibles?}\\
	Según la documentación de phoronix \cite{phoronix}, el comando para ver que benchmarks hay disponibles es el siguiente:\\
	$phoronix$-$test$-$suite$ list-available-tests\\
	Algunos de los tests que están disponibles son los siguientes:\\
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\linewidth]{phoronix-tests}
		\caption[phoronix-tests]{Lista de los benchmarks que tiene disponible phoronix}
		\label{fig:phoronix-tests}
	\end{figure}
	
	\item \textbf{Seleccione, instale y ejecute uno, comente los resultados.}\\
	El benchmark que hemos descargado es el que mide el uso de batería: $battery$-$power$-$usage$. Para instalarlo, ejecutamos el comando $install$ de phoronix. Para ejecutarlo utilizamos el comando $run$. Al ejecutarlo, se nos dice con que nombre queremos que se guarden los resultados. Tras esto, ejecuta el benchmark seleccionado. En nuestro caso, el benchmark mide el uso de la batería en sin hacer nada, con la pantalla apagada y reproduciendo un video. La gráfica que muestro el uso que ha hecho nuestro computador durante el mismo es el siguiente:\\
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\linewidth]{Grafico_resultados_phorenix}
		\caption[Grafico benchmark]{Gráfica que muestra el uso de batería durante el benchmark en MiliWatios.}
		\label{fig:Grafico_resultados_phorenix}
	\end{figure}


\end{itemize}

\subsection{Tests de estrés para Webs.}
\subsubsection{Apache Benchmarck}
En este apartado vamos a responder las preguntas relacionadas con Apache Benchmark(comando ab).\\
\begin{itemize}
	\item \textbf{De los parámetros que le podemos pasar al comando ¿Qué significa -c 5 ? ¿y -n 100? Monitorice la ejecución de ab contra alguna máquina (cualquiera) ¿Cuántos procesos o hebras crea ab en el cliente?}\\
	Según la documentación oficial de Apache(la reference al comando ab) \cite{ab}, la opción -$n$ $request$ fija el número de peticiones que se harán al servidor de apache a $request$. La opción -c, a su vez, fija el número de peticiones concurrentes que dejaremos hacer al cliente. \\
	Tras esta explicación, vayamos ahora a monitorizar un equipo. Para ello, desde la máquina cliente ejecutamos el siguiente comando para bombardear a peticiones nuestro servidor:\\
	
	ab -n 1000 -c 10 http://10.0.2.6/\\
	
	el cual envía 1000 peticiones a través de 10 hebras paralelas(peticiones concurrentes). Este es el resultado de nuestro Benchmarking:\\
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\linewidth]{Ejecucion-ab}
		\caption[Ejecucion ab]{Resultados obtenidos a través del comando ab}
		\label{fig:Ejecucion-ab}
	\end{figure}
	
	Estos resultados nos resumen cuanto ha tardado el servidor en atender a cada petición. A primera vista, nos llama la atención que la diferencia entre el máximo y el mínimo tiempo tardado en dar respuesta a las peticiones es muy grande(90 ms). Si nos fijamos en las últimas filas de la terminal, nos damos cuenta que esto es por la existencia de casos extremos, en los que solo el 1\% de las peticiones ha tardado entre 90 y 22 ms. Todas las demás no han llegado a tardar 22 ms.
	
	\item \textbf{Ejecute ab contra las tres máquinas virtuales (desde el SO anfitrión a las máquina virtuales de la red local, en Ubuntu, CentOS y WS) una a una (arrancadas por separado) y muestre y comente las estadísticas. ¿Cuál es la que proporciona mejores resultados? Fíjese en el número de bytes transferidos, ¿es igual para cada máquina?}\\
	
	Vamos a bombardear a cada máquina con las mismas peticiones y el mismo límite de concurrencia. Vamos a lanzar 1000 peticiones con, como mucho, 10 peticiones a la vez.\\
	
	La primera máquina a la que vamos a bombardear con peticiones es Windows. Estos son los resultados:\\
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.6\linewidth]{Windows-ab}
		\caption[Windows ab]{Resultados del bombardeo de peticiones a Windows}
		\label{fig:Windows-ab}
	\end{figure}
	
	La siguiente máquina a bombardear es Ubuntu. Estos son los resultados:\\
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.6\linewidth]{Ubuntu-ab}
		\caption[Ubuntu-ab]{Resultados del bombardeo de peticiones a Ubuntu}
		\label{fig:Ubuntu-ab}
	\end{figure}
	La última máquina a bombardear es CentOs. Estos son los resultados:\\
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.6\linewidth]{CentOS-ab}
		\caption[CentOS ab]{Resultados del bombardeo de peticiones a CentOS}
		\label{fig:Centos-ab}
	\end{figure}
	
	En cada informe, nos dice la cantidad de información que se envía. En este caso, Ubuntu manda mucha información(11.510B de información), Windows envía sólo 689B de información y CentOs, 4.897B. Esto desencadena en que, obviamente, la velocidad que tardará Ubuntu en servir la página será mayor que las otras dos distribuciones, ya que la página en cuestión es más pesada.\\
	Como venimos anunciando, Ubuntu ha tardado más en servir las 1000 peticiones(0.724 seg), seguido muy de cerca, sorprendentemente, por Windows(0.559 seg). CentOS, por el contrario, ha tardado sólo 0.097 seg.\\
	
	
\end{itemize}

\subsubsection{Gatling}
\begin{itemize}
	\item \textbf{¿Qué es Scala? Instale Gatling y pruebe los escenarios por defecto.}\\
	Scala \cite{scala} es un lenguaje de programación que use los conceptos de orientación a objetos y los funcionales, formando un lenguaje multiparadigma. Está diseñado para expresar patrones de programación y es altamente tipado.\\
	Tras esta explicación, veamos ahora cómo usar la herramienta gatling para estresar el sistema. Según la guia de inicio rápido suministrada por el proyecto gatling \cite{gatling}, para instalar el programa simplemente hay que descomprimir un archivo que te descargas de la misma página para tener el programa en funcionamiento. En el directorio $./bin$ se encuentran los ejecutables de la aplicación. Para correr la aplicación, ejecutamos el ejecutable $./bin/gatling.sh$(en Linux) y nos saldrá una muestra de los tests de ejemplo. El que nosotros ejecutaremos será el test básico. Al terminar, nos dirá que para ver los resultados abramos un link creado en el directorio $./results$. Una de las tablas que crea gatling es la siguiente, donde muestra lo que tarda el sistema en responder a una query de la base de datos.\\
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\linewidth]{Gatling_results1}
		\caption[Table resultados]{Tabla de resultados del test de estrés donde gatling mide lo que tarda el sistema en responder a una petición.}
		\label{fig:Gatling_results1}
	\end{figure}
	
	Y aquí los gráficos generados:\\
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\linewidth]{NumeroRequestGatling}
		\caption[Peticiones por segundo]{Gráfico que muestra el número de peticiones por segundo en el transcurso del test.}
		\label{fig:NumeroRequestGatling}
	\end{figure}
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\linewidth]{NumeroRespuestasGatling}
		\caption[Respuestas gatling]{Gráfico que muestra el número de respuestas por segundo en el transcurso del test.}
		\label{fig:NumeroRespuestasGatling}
	\end{figure}
	
\end{itemize}
\subsubsection{Jmeter}
\begin{itemize}
	\item \textbf{Lea el artículo y elabore un breve resumen\cite{benchmarkingJmGt}.}\\
	El artículo pretende poner cara a cara las prestaciones de la herramienta vista anteriormente(Gatling) y Jmeter, una herramienta parecida. Al inicio del artículo deja muy claro que no quieren poner una por encima de la otra, ya que no es la intención de la página(flood.io), que apoya a ambas, eliminar una en función de la otra. Esto es porque piensan que estas herramientas necesitan distintos requerimientos.\\
	
	Los primeros párrafos describen en que circunstancias se van a llevar a cabo las pruebas con ambas herramientas y por qué. Utilizan, por ejemplo, un servidor con 4 CPUs virtuales en el que han habilitado 15 GB de RAM para asegurarse de que no se producen cuellos de botella en el servidor. También se dejan claros otros detalles, como que se correrá sobre la máquina virtual de Java con una serie de opciones de la misma(aquí no especificadas).\\
	
	La rutina que seguirá el benchmark consistirá en distintas transacciones hechas por "usuarios", cada una de ellas con distintos requerimientos usando recursos mas o menos lentos para tener una idea de todos los tipos de peticiones que se pueden hacer. \\
	
	Los últimos parámetros que se tienen en cuenta son la cantidad de peticiones, el nivel de concurrencia y el tiempo del mismo benchmark. Se correrán 10.000 hebras usuario, 30.000 peticiones por minuto y se correrá durante 20 minutos, con un tiempo de descanso de la máquina de 10 minutos.\\
	
	En este test, puesto que hay varias versiones de Jmeter, se ha pasado a medir las prestaciones de una y otra versión. Nosotros solo evaluaremos la diferencia que hay entre las dos herramientas(Gatling y Jmeter) no entre las distintas versiones de Jmeter.\\
	
	A continuación, pasamos a describir los resultados:\\
	
	\begin{itemize}
		\item Para empezar, la media que calculaba cada uno de los tests era muy parecida, al igual que la desviación típica, por tanto podemos coincidir en que se obtienen casi los mismos resultados con ambos testers.
		\item En cuanto a uso de red, cabe notar que Gatling no guarda información sobre el tamaño de la respuesta. Sin embargo, flood.io usa una estimación basándose en las cabeceras. Aun así, la estimación es optimista. El siguiente, es un gráfico del uso de cada herramienta de la red:\\
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.7\linewidth]{Gatling-Jmeter-Net}
			\caption[Uso de red]{Uso de red de Gatling y las dos versiones de Jmeter.}
			\label{fig:Gatling-Jmeter-Net}
		\end{figure}

		\item Jmeter usa más recursos en la máquina virtual de Java.
		
		\item Jmeter usa más la CPU y más memoria. El siguiente gráfico el porcentaje de utilización de la CPU:\\
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.7\linewidth]{UsoCPU-JmeterGatling}
			\caption[Uso CPU]{Uso de la CPU de Gatling y ambas versiones de Jmeter}
			\label{fig:UsoCPU-JmeterGatling}
		\end{figure}

		\item Ambas herramientas tienen tiempos de respuesta muy parecidos(desviación típica muy pequeña), lo cual, según el artículo, este parámetro no tiene por qué tenerse en cuenta en los tiempos 
		\item Ambas herramientas se mantienen estables en el momento en el que reciben 30.000 peticiones por minuto.
		\item Ambas herramientas pueden aguantar a 10.000 usuarios concurrentemente, lo cual se suele considerar una cantidad agresiva.
	\end{itemize}
	
	Resumiendo, ambas herramientas son muy parecidas, salvo en pequeñas diferencias, como son la medición de uso de red(que Gatling no hace, pero es fácil de medir con otro tester) y el uso de CPU(Jmeter usa ligeramente más). La conclusión es que el uso de una u otra herramienta termina siendo una cuestión subjetiva que dependerá del usuario de las mismas.
	
	
	\item \textbf{Instale y siga el tutorial en \cite{Jmeter} realizando capturas de pantalla y comentándolas. En vez de usar la web de jmeter, haga el experimento usando alguna de sus máquinas virtuales (Puede hacer una página sencilla, usar las páginas de phpmyadmin, instalar un CMS, etc.).}\\
	
	Para empezar, vamos a instalar Jmeter. Para ello, simplemente tendremos que descargar el archivo comprimido que se descarga de la misma página \cite{Jmeter} y descomprimirlo. Tras verificar que tenemos los requerimientos necesarios \cite{Jmeter2}, vamos al manual para crear test para páginas web \cite{Jmeter3}.\\
	
	
	Nuestro test consistirá en crear cinco usuarios que mandarán peticiones a páginas web. Por último, para poder crear estos test de páginas web vamos a necesitar cuatro elementos principales: Thread Group, HTTP Request, HTTP Request Defaults, y Graph Results.\\
	
	\begin{itemize}
		\item Para el primer elemento(Thread Group) vamos a ir a la interfaz para crear un grupo de hilos:\\
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.7\linewidth]{ThreadGroup}
			\caption[Pestaña]{Pestaña donde se encuentra la interfaz para crear el grupo de hilos.}
			\label{fig:ThreadGroup}
		\end{figure}
		
		Luego, en la interfaz, vamos a modificar algunos valores por defecto. El número de hilos lo modificamos a 5. Además, el numero de repeticiones que vamos a exigir es de 2. Así debería quedar:\\
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.7\linewidth]{JmeterThreadGroup}
			\caption[Jmeter thread group]{Configuración de Jmeter deseada.}
			\label{fig:JmeterThreadGroup}
		\end{figure}
		
		\item Ahora vamos a crear los valores por defecto de las peticiones que se harán. Para ello, clicamos con el botón derecho sobre el grupo de hilos creado -> Configuración por defecto -> Valores por defecto para configuración HTTP. Nosotros solo modificaremos el campo IP y el puerto, a la máquina que queramos testear. En nuestro caso, a la máquina con IP 10.0.2.15, al puerto 80. Aquí se ven los cambios realizados:\\
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.7\linewidth]{HTTPPorDefecto}
			\caption[Valores por defecto HTTP]{Valores por defecto modificados en Jmeter}
			\label{fig:HTTPPorDefecto}
		\end{figure}
		
		Jmeter también nos da la opción de incluir Cookies, pero como nuestra página no las usa, nos saltaremos este paso de la guía.
		
		\item Con el objeto creado anteriormente solo hemos modificado los valores por defecto, pero no habremos mandado ninguna petición. Ahora si vamos a crear la propia petición. Para ello, clicamos con el botón derecho sobre nuestro grupo de hilos y le damos a Añadir->Muestreador->Petición HTTP. En ella lo único que necesitaremos cambiar es el nombre, si queremos, y la ruta. No necesitamos especificar la ruta del servidor, ya que ya la tenemos configurada en las opciones por defecto. Así quedaría la petición HTTP:\\
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.7\linewidth]{PeticionHTTP}
			\caption[Petición HTTP]{Valores modificados de la petición HTTP.}
			\label{fig:PeticionHTTP}
		\end{figure}
		
		\item Por último, tenemos que añadir un elemento que escuche. Este elemento será el responsable de mostrar los resultados de la ejecución de Jmeter, que en nuestro caso será un gráfico. Para ello, clicamos de nuevo enel grupo de hilos y vamos a Añadir->Receptor->Grafico de Resultados. Tras esto, debemos especificar el nombre del archivo al que se volcarán los resultados. Esta es la interfaz que ofrece:\\
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.7\linewidth]{GraficosJmeter}
			\caption[Interfaz Gráficos]{Interfaz que ofrece Jmeter para creación de gráficos.}
			\label{fig:GráficosJmeter}
		\end{figure}
		
		Tras esto, ejecutamos varias veces el test y este es el gráfico que nos queda:\\
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.7\linewidth]{GraficoJmeter}
			\caption[Grafico Jmeter]{Resultados de la ejecución de Jmeter}
			\label{fig:GraficoJmeter}
		\end{figure}
		
		En la parte de abajo del gráfico, vemos valores clave del mismo gráfico, entre ellos la media y la mediada. Según estos, la mitad de las peticiones, que han sido 700, se han llevado a cabo en un tiempo menor a 4 ms, con una media total de 6ms. También vemos en la gráfica que las primeras peticiones tardaban más, pero que al final el tiempo de respuesta y todos los parámetros que en el intervienen se estabilizan.

	\end{itemize}
	
	\subsection{Benchmark}
	En esta última parte de la memoria, tenemos que programar un benchmark e incluir el objetivo del mismo, que unidades, variables o puntuaciones utilizará, instrucciones de uso y un ejemplo en el que se analizen los resultados.\\
	
	Para la realización del benchmark se ha consultado el benchmark publicado el año pasado por el compañero de clase Oscar \cite{oscar} y se ha modificado. A continuación se explican los campos que se requieren:\\
	
	\begin{itemize}
		\item Para comenzar, el Benchmark pretende medir los MFLOPS del computador en el que se ejecuten. Para ello, la rutina que ejecutará será la de invertir una matriz de dimensión 1000 aleatoria por el método de descomposición LU y se calcularán las operaciones en coma flotante necesarias para ella. Se medirá el tiempo necesario en la inversión y, a partir de los datos de tiempo y número de operaciones, se calculñarán los MFLOPS.
		\item Para que el benchmark sea más homogeneo, este calculo se hará 5 veces y se calculará la media. 
		
		\item La forma de ejecución de este Benchmark será simplemente ejecutar el script benchmark.sh, que sacará por pantalla el número de MFLOPS del computador. El funcionamiento de este script es el siguiente: primero compila los programas necesarios para la ejecución(el que invierte la matriz y el que calcula la media de los datos). Tras esto, llama y guarda los resultados de la ejecución del programa que calcula la inversa de la matriz las cinco veces dichas anteriormente. Tras esto, calcula la media de estos valores y lo muestra en pantalla.
	\end{itemize}
	
	Se pasa ahora a analizar los resultados de ejecutar el script en varias computadoras:\\
	
	\begin{center}
		\begin{tabular}{| l || r | }
			\hline
			Procesador & MFLOPS\\ \hline \hline
			Intel® Core™ i7-3630QM CPU @ 2.40GHz × 8  & 497.205 \\ \hline
			7 & 9 \\
			\hline
		\end{tabular}
	\end{center}
	
\end{itemize}



\newpage

\begin{thebibliography}{xx}
	\bibitem{phoronix} http://www.phoronix-test-suite.com/documentation/phoronix-test-suite.pdf
	\bibitem{ab} https://httpd.apache.org/docs/2.4/programs/ab.html
	\bibitem{scala} http://www.scala-lang.org/
	\bibitem{gatling} http://gatling.io/docs/2.2.1/quickstart.html
	\bibitem{benchmarkingJmGt} https://blog.flood.io/benchmarking-jmeter-and-gatling/
	\bibitem{Jmeter} http://jmeter.apache.org/download\_jmeter.cgi
	\bibitem{Jmeter2} http://jmeter.apache.org/usermanual/get-started.html
	\bibitem{Jmeter3} http://jmeter.apache.org/usermanual/build-web-test-plan.html
	\bibitem{oscar} https://github.com/oxcar103/Benchmark-103
	\bibitem{necesario}
	https://repo1.maven.org/maven2/io/gatling/highcharts/gatling-charts-highcharts-bundle/2.1.7/gatling-charts-highcharts-bundle-2.1.7-bundle.zip
	
	
\end{thebibliography}
\end{document}